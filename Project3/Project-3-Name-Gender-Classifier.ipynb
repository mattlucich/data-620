{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "588c372d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import random\n",
    "import re\n",
    "from nltk.corpus import names\n",
    "from nltk.metrics import ConfusionMatrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8efbc636",
   "metadata": {},
   "source": [
    "### Load and Format Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "59126a77",
   "metadata": {},
   "outputs": [],
   "source": [
    "#nltk.download('names')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef678c9a",
   "metadata": {},
   "source": [
    "The dataset is imbalanced with approximately 63% of the names being female. Yet the imbalance is not severe enough to consider downsampling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "id": "8191cb29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of male names: 2943\n",
      "Number of female names: 5001\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of male names:\", len(names.words('male.txt')))\n",
    "print(\"Number of female names:\", len(names.words('female.txt')))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "466d9e81",
   "metadata": {},
   "source": [
    "We convert the text into a list of tuples, with each tuple containing a name and the associated gender. This format will make it easier to extract features and eventually fit nltk models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "id": "2361bd1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "labeled_names = ([(name, 'male') for name in names.words('male.txt')] + [(name, 'female') for name in names.words('female.txt')])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "948e011e",
   "metadata": {},
   "source": [
    "Since the labeled_names list contains male names and then female names sequentially we shuffle the list of tuples so when we create our training and test sets there is an approximate balance of genders in each set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "id": "9de6dbdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "random.shuffle(labeled_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e740026",
   "metadata": {},
   "source": [
    "### Create Word Features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8bcb8b0",
   "metadata": {},
   "source": [
    "Specify all features we want to extract from the names and include into our models. The only feature we tested but did not include is the number of vowels in the name. It was not included since in consistently decreased accuracy. The features included are: last letter, last two letters, last three letters, first letter, first two letters, first three letters, and word length. The general form of this function comes from examples in [chapter 6](https://www.nltk.org/book/ch06.html) of *Natural Language Processing with Python*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "id": "822a6148",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gender_features(word):\n",
    "        return {'last_letter': word[-1], \n",
    "            'last_two_letters': word[-2:],\n",
    "            'last_three_letters': word[-3:],\n",
    "            'first_letter': word[:1],\n",
    "            'first_two_letters': word[:2],\n",
    "            'first_three_letters': word[:3],\n",
    "            'word_length': len(word)}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c519195",
   "metadata": {},
   "source": [
    "Using the function above, for each name in the labeled_names list we create a tuple with a dictionary of features and the gender."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "id": "c6ae4b5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "featuresets = [(gender_features(n), gender) for (n, gender) in labeled_names]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "591b37a4",
   "metadata": {},
   "source": [
    "### Create Train, Dev Test, and Test Sets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cfc2f89",
   "metadata": {},
   "source": [
    "Create dev_test_set, test_set, train_set datasets as specified in the project instructions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "id": "f5369e4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_test_set, test_set, train_set = featuresets[:500], featuresets[500:1000], featuresets[1000:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b909ef35",
   "metadata": {},
   "source": [
    "Confirm the length of each set is correct. We note a small discrepancy between the expected 6900 words in the train_set and the actual lenght of 6944."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "id": "c99e83e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500\n",
      "500\n",
      "6944\n"
     ]
    }
   ],
   "source": [
    "print(len(dev_test_set))\n",
    "print(len(test_set))\n",
    "print(len(train_set))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1864559",
   "metadata": {},
   "source": [
    "### Naive Bayes Classifier (final model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "id": "4ffda6f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = nltk.NaiveBayesClassifier.train(train_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d28f5aea",
   "metadata": {},
   "source": [
    "#### Accuracy of Dev Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "id": "024e862a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.854"
      ]
     },
     "execution_count": 281,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.classify.accuracy(classifier, dev_test_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "767ec15c",
   "metadata": {},
   "source": [
    "#### Accuracy of Test Set (final predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd136209",
   "metadata": {},
   "source": [
    "We use accuracy as our evaluation metric since there is no preference to type I or type II errors. Further, we see from the confusion matrix below that there is not a strong bias towards false positives or false negatives. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "id": "d9f2f8b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.836"
      ]
     },
     "execution_count": 282,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.classify.accuracy(classifier, test_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8427b704",
   "metadata": {},
   "source": [
    "#### Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "id": "8db4f59c",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_genders = [x[-1] for x in test_set]\n",
    "test_features = [x[:-1] for x in test_set]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "id": "36cce642",
   "metadata": {},
   "outputs": [],
   "source": [
    "classify_test = [classifier.classify(x[0]) for x in test_features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "id": "9bfbf51d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       |   f     |\n",
      "       |   e     |\n",
      "       |   m   m |\n",
      "       |   a   a |\n",
      "       |   l   l |\n",
      "       |   e   e |\n",
      "-------+---------+\n",
      "female |<269> 47 |\n",
      "  male |  35<149>|\n",
      "-------+---------+\n",
      "(row = reference; col = test)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cm = ConfusionMatrix(test_genders, classify_test)\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c83d47e3",
   "metadata": {},
   "source": [
    "#### Most Important Features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5d3e16c",
   "metadata": {},
   "source": [
    "Here we review the most informative features and see that the last two letters of a name tend to dominate the usefulness in predicting gender. Nevertheless, we keep all features (except for number of vowels) in the model as they all contribute to accuracy as determined through iterative testing. We also uncover somewhat interesting insights such as names ending in 'na' and 'la' have likelihood ratios of 93.6 and 69.3, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "id": "3d432c20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most Informative Features\n",
      "        last_two_letters = 'na'           female : male   =     93.6 : 1.0\n",
      "        last_two_letters = 'la'           female : male   =     69.3 : 1.0\n",
      "        last_two_letters = 'ld'             male : female =     39.5 : 1.0\n",
      "             last_letter = 'a'            female : male   =     37.1 : 1.0\n",
      "        last_two_letters = 'ia'           female : male   =     36.4 : 1.0\n",
      "        last_two_letters = 'ra'           female : male   =     34.2 : 1.0\n",
      "             last_letter = 'k'              male : female =     29.9 : 1.0\n",
      "        last_two_letters = 'rt'             male : female =     29.7 : 1.0\n",
      "        last_two_letters = 'us'             male : female =     27.4 : 1.0\n",
      "      last_three_letters = 'ana'          female : male   =     25.2 : 1.0\n"
     ]
    }
   ],
   "source": [
    "classifier.show_most_informative_features(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60295004",
   "metadata": {},
   "source": [
    "### Other Classifiers Tested (but not chosen for final model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06c0677a",
   "metadata": {},
   "source": [
    "#### Decision Trees Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "id": "a58a53ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_classifier = nltk.DecisionTreeClassifier.train(train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "id": "03e4b768",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.74"
      ]
     },
     "execution_count": 288,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.classify.accuracy(dt_classifier, dev_test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "id": "7a1ee32e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.726"
      ]
     },
     "execution_count": 289,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.classify.accuracy(dt_classifier, test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "id": "354d7129",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(dt_classifier.pseudocode(depth=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84b0908d",
   "metadata": {},
   "source": [
    "#### Maximum Entropy Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "id": "40aa8175",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ==> Training (100 iterations)\n"
     ]
    }
   ],
   "source": [
    "me_classifier = nltk.MaxentClassifier.train(train_set, trace=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "id": "df8ec84a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.834"
      ]
     },
     "execution_count": 292,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.classify.accuracy(me_classifier, dev_test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "id": "977e5792",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.828"
      ]
     },
     "execution_count": 293,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.classify.accuracy(me_classifier, test_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dc6bda7",
   "metadata": {},
   "source": [
    "### How does the performance on the test set compare to the performance on the dev-test set? Is this what you'd expect?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69362171",
   "metadata": {},
   "source": [
    "Generally, when comparing only training set performance against test set performance I see worse performance in the test set. The degree of the degraded performance depends heavily on the situation as overfitting occurs more commonly with different models and datasets. When comparing dev test set performance with test set performance the difference in evaluation metrics is expected to be less since the model was not fitted on either set of data. However, if one is continually tweaking a model and validating those tweaks on the dev test set, then one would expect degraded performance on the test set. Since for this project, not too much feature tweaking and model parameter tweaking was needed due to the relative simplicity of the classification task, the accuracy of the two sets do not deviate significantly and the direction in which they deviate appears to be random."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f5af87d",
   "metadata": {},
   "source": [
    "### References"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2cb3bc9",
   "metadata": {},
   "source": [
    "*Natural Language Processing with Python* by Steven Bird, Ewan Klein, and Edward Loper<br> \n",
    "nltk.classify.MaxentClassifier.train: https://tedboy.github.io/nlps/generated/generated/nltk.classify.MaxentClassifier.train.html"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
